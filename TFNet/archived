## Lat: Latitude
## Lon: Longitude
## Time: 
## Level: hecto-Pascals
## z:  geopotential | Proportional to the height of a pressure level | [m^(2) s^(−2)] | 13 levels
    ## describes height in meters and g = 9.81 m s^(−2) is the gravitational acceleration
## pv: Potential vorticity
    ## used quantity in synoptic meteorology which combines the rotation (vorticity) and vertical temperature
    ## gradient of the atmosphere
## r: 
## q: 
## t: temperature | Temperature |
## u:
## vo: 
## v: 
## u10: 
## v10: 
## t2m: 2 meter-temperature
    ##  used as an impact variable because of its relevance for human activities and 
    ##  is directly affected by the diurnal solar cycle
## tisr: 
## tcc: 
## tp: 









## TFNet Train Model 
## Date: 28 Feb 2023
## Contributors: Alexander Christopher, Werner Hager


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
#import model as m
import test_model as m



#####################################
## Import the dataset


filepath = "../Data/df_week.csv"
#filepath = "../Data/one_day_testing.csv"
data = pd.read_csv(filepath)

## Lat: Latitude
## Lon: Longitude
## Time: 
## Level: hecto-Pascals
## z:  geopotential                     | Proportional to the height of a pressure level    | [m^(2) s^(−2)]            | 13 levels
## pv: potential_vorticity              | Potential vorticity                               | [K m^(2) kg^(-1) s^(-1)]  | 13 levels
## r: relative_humidity                 | Humidity relative to saturation                   | [%]                       | 13 levels
## q: specific_humidity                 | Mixing ratio of water vapor                       | [kg kg^(−1)]              | 13 levels
## t: temperature                       | Temperature                                       | [K]                       | 13 levels
## u: u_component_of_wind               | Wind in x/longitude-direction                     | [m s^(-1)]                | 13 levels
## vo: vorticity                        | Relative horizontal vorticity                     | [1 s^(-1)]                | 13 levels
## v: v_component_of_wind               | Wind in y/latitude direction                      | [m s^(-1)]                | 13 levels
## u10: 10m_u_component_of_wind         | Wind in x/longitude-direction at 10 m height      | [m s^(-1)]                | Sinlge
## v10: 10m_v_component_of_wind         | Wind in y/latitude-direction at 10 m height       | [m s^(-1)]                | Sinlge
## t2m: 2m_temperature                  | 2m_temperature                                    | [K]                       | Single
## tisr: toa_incident_solar_radiation   | Accumulated hourly incident solar radiation       | [J m^(-2)]                | Sinlge
## tcc: total_cloud_cover               | Fractional cloud cover                            | (0-1)                     | Sinlge
## tp: total_precipitation              | Hourly precipitation                              | [m]                       | Single


    ## Positive u_component_of_wind --> Wind coming from the West
    ## Negative u_component_of_wind --> Wind coming from the East
    ## Positive v_component_of_wind --> Wind coming from the South
    ## Negative v_component_of_wind --> Wind coming from the North
    ## Wind Speed = sqrt(U*U + V*V)
    ## Wind Direction Angle = arctan(V/U


#####################################


#####################################
## Clean the dataset 


## Drop Columns
    ## 'Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1'
data = data.drop(['Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1'], axis = 1)
data = data.drop(['Unnamed: 0_x', 'Unnamed: 0_y'], axis = 1)

## Useful Indexing:
    ## Same lat, log, level --> Unamed: 0_x
    ## Same lat, log, time --> Unamed: 0_y

## Check for null values
data = data.dropna()


#####################################


#####################################
## Visualization


# Find lat, long, and level and compare the graph the u_component_of_wind
"""lat, long, low_level, high_level = 47.8125, 67.5, 50, 1000

low_df = data[(data["lat"] == lat) & (data["lon"] == long) & (data["level"] == low_level)] 
low_df.plot(x = "time", y = "u")
plt.ylabel("u_component_of_wind")
plt.xlabel("time")


low_df.plot(x = "time", y = "tcc")
plt.ylabel("Total Cloud Cover")
plt.xlabel("time")

low_df.plot(x = "time", y = "tp")
plt.ylabel("Total Percipation")
plt.xlabel("time")

high_df = data[(data["lat"] == lat) & (data["lon"] == long) & (data["level"] == high_level)] 
high_df.plot(x = "time", y = "u")
plt.ylabel("u_component_of_wind")
plt.xlabel("time")
plt.show()"""


#####################################


#####################################
## Preprocessing 


## Generating validation and test dataset
#start_val_date = "2018-01-06 00:00:00"
#end_val_date = "2018-01-06 23:00:00"
start_test_date = "2018-01-07 00:00:00"
end_test_date = "2018-01-07 23:00:00"

#train_data = data[start_val_date > data["time"]]
#validation_data = data[(start_val_date <= data["time"]) & (data["time"] <= end_val_date)]
validation_data = None
train_data = data[start_test_date > data["time"]]
test_data = data[(start_test_date <= data["time"]) & (data["time"] <= end_test_date)]


#####################################


#####################################
## Train Model


model = m.Model(train_data, validation_data, test_data)

## base model
base_model = model.base()

## TFNet
tfnet_model = model.tfnet()


#####################################


#####################################
## Testing Evaluation & Visualization





#####################################












## TFNet Model
## Date: 28 Feb 2023
## Contributors: Alexander Christopher, Werner Hager


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

import IPython
import IPython.display
import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import tensorflow as tf


class Model():
    ## User picks lat, lon, time
    ## Sea Level: level = 1000
    ## Set level == 1000 --> temperature (t) & relative humidity (r) set to level 1000
    ## total cloud cover (tcc) & total prepicpation (tp) single level no need to set

    ## Predictive Attributes
    #pred_cols = ['lat', 'lon', 'time', 'level', 'z', 'pv', 'r', 'q', 't',
    #    'u', 'vo', 'v', 'u10', 'v10', 't2m', 'tisr', 'tcc',
    #    'tp']

    ## Target Attributes
    #tar_cols = ['lat', 'lon', 'time', 'level', 't', 'r', 'tcc', 'tp']
    #tar_level = 1000
    #target_cols = ['lat', 'lon', 'time', 't2m', 'tcc', 'tp']

    def __init__(self, train, validation, test) -> None:
        seed = 4
        tf.random.set_seed(seed)
        np.random.seed(seed)
        self.baseTrain = train
        self.baseValidation = validation
        self.baseTest = test

    def dataConversion(self):
        ## Positive u_component_of_wind --> Wind coming from the West
        ## Negative u_component_of_wind --> Wind coming from the East
        ## Positive v_component_of_wind --> Wind coming from the South
        ## Negative v_component_of_wind --> Wind coming from the North
        ## Wind Speed = sqrt(U*U + V*V)
        ## Wind Direction Angle = arctan(V/U)
        pass

    def base(self):
        date_time = pd.to_datetime(self.baseTrain.pop('time'), format= "%Y-%m-%d %H:%M:%S")
        #location = 

        ## Raw Data Visualization
        """plot_cols = ['z', 'pv', 'r', 'q', 't', 'u', 'vo', 'v']
        plot_features = self.train[plot_cols]
        plot_features.index = date_time
        _ = plot_features.plot(subplots=True)
        plt.show()"""

        ## Data Transposing Visualization
        print(self.baseTrain.describe().transpose())
        print()


        ## Feature Engineering
        ## Columns u, v in vector form already

        ## Converting time into seconds
        timestamp_s = date_time.map(pd.Timestamp.timestamp)

        ## Converting timestamps_s into periodicity interpertable info
        hour = 60*60
        day = 24*hour
        year = 365.2425 * day 

        #self.baseTrain['Hour sin'] = np.sin(timestamp_s * (2 * np.pi / hour))
        #self.baseTrain['Hour cos'] = np.cos(timestamp_s * (2 * np.pi / hour))
        self.baseTrain['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))
        self.baseTrain['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))
        self.baseTrain['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))
        self.baseTrain['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))

        # visualization of new attributes
        """plt.plot(np.array(self.baseTrain['Day sin'])[:500])
        plt.plot(np.array(self.baseTrain['Day cos'])[:500])
        plt.xlabel('Time [h]')
        plt.title('Time of day signal')
        plt.show()"""

        ## Visualization of geolocation
        """plt.figure(figsize = (10,7))
        sns.scatterplot(data=self.baseTrain, x='lon', y='lat')
        plt.show()"""

        ## most important frequency features
        """fft = tf.signal.rfft(self.baseTrain['t'])
        f_per_dataset = np.arange(0, len(fft))

        n_samples_h = len(self.baseTrain['t'])
        hours_per_year = 24*365.2524
        years_per_dataset = n_samples_h/(hours_per_year)

        f_per_year = f_per_dataset/years_per_dataset
        plt.step(f_per_year, np.abs(fft))
        plt.xscale('log')
        plt.ylim(0, 4000000)
        plt.xlim([0.1, max(plt.xlim())])
        plt.xticks([1, 12, 52, 365.2524, 24*365.2524], labels=['1/Year', '1/month', '1/week', '1/day', '1/hour'])
        _ = plt.xlabel('Frequency (log scale)')
        plt.show()"""

        ## Normalize the dataset
        # remove lat and lon columns before normalization


        ## Categorize u10, v10, t2m, tisr, tcc, tp by location and time --> Normalize separately 
        ## Split data by level, lat, and log --> normalize each by their respective location, level, and time --> put back together
        ## Add u10, v10, t2m, tisr, tcc, tp back into the dataset



        base_train_ltlg = self.baseTrain[['lat', 'lon', 'level']]
        self.baseTrain = self.baseTrain.drop(['lat', 'lon', 'level'], axis = 1)
        #base_validation_ltlg = self.baseValidation.pop(['lat', 'lon'])
        #base_test_ltlg = self.baseTest.pop(['lat', 'lon'])

        # normalize datasets with moving average
        base_train_f5 = self.baseTrain.head(5)
        base_train = self.baseTrain.rolling(window=5).mean()
        print(base_train.head(10))

        # add lat and lon columns after normalization
        base_train = None 
        base_validation = None
        base_test = None

        ## Split into training and validation

        ## Data Windowing



        ## Encoder --> keep it in a single form --> throw it into time series --> done



        ## **** Add Smoothing *******


    def tfnet(self):
        pass
